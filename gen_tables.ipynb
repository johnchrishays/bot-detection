{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99af18c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T19:47:13.219048Z",
     "start_time": "2022-10-01T19:47:12.867801Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from biblib import Entry\n",
    "import pybtex as pbt\n",
    "import math\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec8925",
   "metadata": {},
   "source": [
    "## Load data needed to generate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec96f398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T02:29:09.742949Z",
     "start_time": "2022-10-02T02:29:09.731993Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '~/Downloads/Simple Decision Rules Give High Accuracy for Detecting Social Bots on Benchmark Datasets - Bot detection papers(14).tsv'\n",
    "#path = 'metrics.tsv'\n",
    "\n",
    "df = pd.read_csv(path, sep='\\t')\n",
    "df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cd0939d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T22:04:02.271161Z",
     "start_time": "2022-10-02T22:04:02.262527Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df_path = '~/Downloads/Simple Decision Rules Give High Accuracy for Detecting Social Bots on Benchmark Datasets - datasets(8).tsv'\n",
    "\n",
    "dataset_df = pd.read_csv(dataset_df_path, sep='\\t')\n",
    "dataset_df.fillna(\"\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf58bb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T19:47:15.915779Z",
     "start_time": "2022-10-01T19:47:15.909961Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_path = '~/work/repo/bot-detection/scores.csv'\n",
    "sdt_df = pd.read_csv(scores_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfdb940",
   "metadata": {},
   "source": [
    "## Generate bibliography methods.bib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5477a63e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T22:04:07.386180Z",
     "start_time": "2022-10-02T22:04:07.334976Z"
    }
   },
   "outputs": [],
   "source": [
    "bib = pbt.database.BibliographyData()\n",
    "\n",
    "def add_bib_entries(df):\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        #print(row)\n",
    "        if row['bibtex_id'] in bib.entries.keys():\n",
    "            continue\n",
    "        if 'analyzed?' in row:\n",
    "            if not row['analyzed?']:\n",
    "                continue\n",
    "        if not row['bibtex_id']:\n",
    "            continue\n",
    "        inputdict = {\n",
    "            'author': row['authors'],\n",
    "            'title': row['title'],\n",
    "            'year': str(int(row['year']))\n",
    "        }\n",
    "\n",
    "        if row['conference?']:\n",
    "            inputdict.update({\n",
    "                'booktitle': row['booktitle'],\n",
    "                'pages': row['pages'],\n",
    "            })\n",
    "            if row['booktitle']:\n",
    "                inputdict['booktitle'] = row['booktitle'] \n",
    "            type_ = 'inproceedings'\n",
    "        else:\n",
    "            inputdict['journal'] = row['journal']\n",
    "            if row['volume']:\n",
    "                inputdict['volume'] = str(int(row['volume']))\n",
    "            if row['number']:\n",
    "                inputdict['number'] = str(int(row['number']))\n",
    "            type_ = 'article'\n",
    "        if row['publisher']:\n",
    "            inputdict['publisher'] = row['publisher']\n",
    "        if row['doi']:\n",
    "            inputdict['doi'] = row['doi']\n",
    "        if row['pages']:\n",
    "            inputdict['pages'] = row['pages']\n",
    "        entry = pbt.database.Entry(type_=type_, fields=inputdict)\n",
    "        bib.add_entry(entry=entry, key=row['bibtex_id'])\n",
    "\n",
    "add_bib_entries(df)\n",
    "add_bib_entries(dataset_df)\n",
    "\n",
    "bib.to_file(\"methods.bib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620287bc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate table dataset -> paper that uses it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd6e603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T19:47:21.098519Z",
     "start_time": "2022-10-01T19:47:21.092409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "\n",
    "for row in df.to_dict(orient=\"records\"):\n",
    "    datasets = row['dataset(s) used'].split(\"; \")\n",
    "    for d in datasets:\n",
    "        if d in dataset_dict:\n",
    "            dataset_dict[d].append(row['bibtex_id'])\n",
    "        else:\n",
    "            dataset_dict[d] = [row['bibtex_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85826edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T19:47:21.850074Z",
     "start_time": "2022-10-01T19:47:21.840011Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twibot-2020 & \\cite{feng2022heterogeneity-aware, , , feng2021botrgcn, geng2021satar, dehghan2018detecting} \\\\\n",
      "cresci-rtbust-2019 & \\cite{guo2022social, , yang2020scalable, sayyadiharikandeh2020detection, , , , mazza2019rtbust} \\\\\n",
      "botometer-feedback-2019 & \\cite{guo2022social, , yang2020scalable, sayyadiharikandeh2020detection} \\\\\n",
      "gilani-2017 & \\cite{guo2022social, dimitriadis2021social, , yang2020scalable, gilani2020classification, sayyadiharikandeh2020detection, , echeverria2018lobo} \\\\\n",
      "cresci-stock-2018 & \\cite{guo2022social, dimitriadis2021social, , , yang2020scalable, sayyadiharikandeh2020detection} \\\\\n",
      "midterm-2018 & \\cite{guo2022social, dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "cresci-2015 & \\cite{gonzalez2022the, , dimitriadis2021social, , stella2019influence, echeverria2018lobo, cresci2015fame} \\\\\n",
      "cresci-2017 & \\cite{gonzalez2022the, thavasimani2022a, heidari2021an, ilias2021detecting, geng2021satar, dimitriadis2021social, , yang2020scalable, heidari2020using, heidari2020deep, sayyadiharikandeh2020detection, , stella2019influence, loyola-gonzalez2019contrast, mohammad2019bot, alhosseini2019detect, knauth2019language-agnostic, kosmajac2019twitter, , kudugunta2018deep, cresci2018social, efthimion2018supervised, echeverria2018lobo, ferrara2017disinformation, cresci2017the, cresci2017exploiting, cresci2016dna-inspired} \\\\\n",
      " & \\cite{, rodr√≠guez-ruiza2020a, braker2020botspot, , , zheng2015detecting, stringhini2010detecting, lee2010uncovering, cresci2018from, chavoshi2017on-demand, gilani2019a, gilani2017of} \\\\\n",
      "caverlee-2011 & \\cite{ilias2021detecting, dimitriadis2021social, yang2020scalable, sayyadiharikandeh2020detection, , alhosseini2019detect, beskow2018bot, varol2017online, wu2017adaptive, davis2016botornot, alarifi2016twitter, lee2011a} \\\\\n",
      "pan-2019 & \\cite{geng2021satar, , luo2019deepbot} \\\\\n",
      "pronbots-2019 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "political-bots-2019 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection} \\\\\n",
      "astroturf & \\cite{dimitriadis2021social, sayyadiharikandeh2020detection} \\\\\n",
      "botwiki-2019 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "vendor-purchased-2019 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "varol-2017 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, , kosmajac2019twitter, varol2017online, ferrara2017disinformation} \\\\\n",
      "italian_elections & \\cite{dimitriadis2021social} \\\\\n",
      "ira-2018 & \\cite{weber2021amplifying} \\\\\n",
      "weber-2020 & \\cite{weber2021amplifying} \\\\\n",
      "verified-2019 & \\cite{, yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "celebrity-2019 & \\cite{, } \\\\\n",
      "cresci-rtbust & \\cite{} \\\\\n",
      "botometer-feedback & \\cite{} \\\\\n",
      "gilani & \\cite{} \\\\\n",
      "midterm & \\cite{} \\\\\n",
      "kaiser & \\cite{sayyadiharikandeh2020detection} \\\\\n",
      "vargas-2020 & \\cite{vargas2020on} \\\\\n",
      "policial-bots-2019 & \\cite{} \\\\\n",
      "russian-trolls-2018 & \\cite{stella2019influence} \\\\\n",
      "cresci-2017 tweets & \\cite{wei2019twitter} \\\\\n",
      "yang-2013-traditional-spambots-1 & \\cite{alhosseini2019detect} \\\\\n",
      "albadi-2018 & \\cite{albadi2019investigating, albadi2018are} \\\\\n",
      "gilani-2017 tweets & \\cite{garcia-silva2019an} \\\\\n",
      "NBC-2018 & \\cite{efthimion2018supervised} \\\\\n",
      "random-strings-2018 & \\cite{beskow2018bot} \\\\\n",
      "journalist-attack-2017 & \\cite{beskow2018bot, echeverria2018lobo} \\\\\n",
      "starwars & \\cite{echeverria2018lobo} \\\\\n",
      "besel-2018 & \\cite{echeverria2018lobo} \\\\\n",
      "debot & \\cite{echeverria2018lobo, chavoshi2016debot} \\\\\n",
      "darpa-challenge-2015 & \\cite{echeverria2018lobo} \\\\\n",
      "mesnards-2018 & \\cite{mesnards2018detecting} \\\\\n",
      "morstatter-2016 & \\cite{cai2017behavior, morstatter2016a} \\\\\n",
      "ersahin-2017 & \\cite{ersahin2017twitter, ahmed2013a} \\\\\n",
      "kantepe-2017 & \\cite{kantepe2017preprocessing} \\\\\n",
      "alarifi-2016 & \\cite{alarifi2016twitter} \\\\\n",
      "wang-2009 & \\cite{miller2014twitter, wang2010dont} \\\\\n",
      "a large dataset relating to the 2014 Indian election & \\cite{dickerson2014using} \\\\\n",
      "yang-2013-traditional-spambots & \\cite{yang2013empirical} \\\\\n",
      "chu-2012 & \\cite{chu2012detecting2, chu2012detecting} \\\\\n",
      "possibly spammers used in cresci-2017 or another spam dataset & \\cite{chao2011die} \\\\\n"
     ]
    }
   ],
   "source": [
    "for k,v in dataset_dict.items():\n",
    "    impl_papers = \", \".join(dataset_dict[k])\n",
    "\n",
    "    cite_as = '\\\\cite{' + impl_papers + '} \\\\\\\\'\n",
    "    dataset_name = k\n",
    "    print(dataset_name+ \" & \" + cite_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ca18b",
   "metadata": {},
   "source": [
    "## Generate table for dataset, #people/bots, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "206f8a3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T15:10:31.482498Z",
     "start_time": "2022-10-02T15:10:31.474402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data{botometer-feedback-2019} & \\cite{yang2019arming} & 380 & 139 & Accounts misclassified by public, manually annotated by PhD student.\\\\\n",
      "\\data{botwiki-2019} & \\cite{yang2020scalable} & 0 & 698 & Self-identified bot accounts.\\\\\n",
      "\\data{caverlee-2011} & \\cite{lee2011a} & 19276 & 22223 & Bots collected by honeypot.\\\\\n",
      "\\data{ (pan-2019)} \\\\\n",
      "\\data{celebrity-2019} & \\cite{yang2019arming} & 5918 & 0 & Celebrity accounts.\\\\\n",
      "\\data{the-fake-project-2015} & \\cite{cresci2015fame} & 469 & 0 & Followers of @TheFakeProject who completed CAPTCHA.\\\\\n",
      "\\data{ (cresci-2015;  pan-2019)} \\\\\n",
      "\\data{elezioni-2015} & \\cite{cresci2015fame} & 1488 & 0 & Twitter users who used hashtag #elezioni2013, excluding politicians, media and parties; manually verified by sociologists.\\\\\n",
      "\\data{ (cresci-2015; pan-2019)} \\\\\n",
      "\\data{fake-followers-2015} & \\cite{cresci2017the} & 0 & 3351 & Fake followers.\\\\\n",
      "\\data{ (cresci-2017; cresci-2015;  pan-2019)} \\\\\n",
      "\\data{genuine-accounts-cresci-2017} & \\cite{cresci2017the} & 3474 & 0 & Random sample of genuine (human-operated) accounts.\\\\\n",
      "\\data{ (cresci-2017;  pan-2019)} \\\\\n",
      "\\data{social-spambots-1-2017} & \\cite{cresci2017the} & 0 & 991 & Retweeters of an Italian political candidate.\\\\\n",
      "\\data{ (cresci-2017;  pan-2019)} \\\\\n",
      "\\data{social-spambots-2-2017} & \\cite{cresci2017the} & 0 & 3457 & Spammers of paid apps for mobile devices.\\\\\n",
      "\\data{ (cresci-2017;  pan-2019)} \\\\\n",
      "\\data{social-spambots-3-2017} & \\cite{cresci2017the} & 0 & 464 & Spammers of products on Amazon.\\\\\n",
      "\\data{ (cresci-2017;  pan-2019)} \\\\\n",
      "\\data{traditional-spambots-1-2013} & \\cite{yang2013empirical} & 0 & 1000 & Accounts spamming malicious links from public blacklists.\\\\\n",
      "\\data{ (cresci-2017;  yang-2013; pan-2019)} \\\\\n",
      "\\data{genuine-accounts-yang-2013} & \\cite{yang2013empirical} & 10000 & - & Random sample of genuine (human-operated) accounts.\\\\\n",
      "\\data{ (yang-2013)} \\\\\n",
      "\\data{traditional-spambots-2-2017} & \\cite{cresci2017the} & 0 & 100 & Spammers of scam URLs.\\\\\n",
      "\\data{ (cresci-2017;  pan-2019)} \\\\\n",
      "\\data{traditional-spambots-3-2017} & \\cite{cresci2017the} & 0 & 403 & Accounts spamming job offers.\\\\\n",
      "\\data{ (cresci-2017;  pan-2019)} \\\\\n",
      "\\data{traditional-spambots-4-2017} & \\cite{cresci2017the} & 0 & 1128 & Accounts spamming job offers.\\\\\n",
      "\\data{ (cresci-2017;  pan-2019)} \\\\\n",
      "\\data{rtbust-2019} & \\cite{mazza2019rtbust} & 340 & 353 & Manually labeled accounts sampled from Italian retweets in 2018.\\\\\n",
      "\\data{cresci-stock-2018} & \\cite{cresci2019fake} & 6174 & 7102 & Bots with coordinated cashtag tweeting patterns.\\\\\n",
      "\\data{gilani-2017} & \\cite{gilani2020classification} & 1939 & 1492 & Manually labeled accounts from accounts with different follower brackets.\\\\\n",
      "\\data{midterm-2018} & \\cite{yang2020scalable} & 8092 & 42446 & Bots with correlated creation and tweeting patterns; humans manually identified from sample of users tweeting about politics.\\\\\n",
      "\\data{pan-2019} & \\cite{rangel2015overview} & 2060 & 2060 & Datasets: caverlee, varol, cresci-2017, cresci-2015 plus ad hoc collection of manually annotated bots, humans from previous author profiling tasks selected from list of influential authors published publicly by 3rd party\\\\\n",
      "\\data{political-bots-2019} & \\cite{yang2019arming} & 0 & 62 & Right-wing American political bots.\\\\\n",
      "\\data{pronbots-2019} & \\cite{yang2019arming} & 0 & 17882 & Bots sharing scam sites.\\\\\n",
      "\\data{twibot-2020} & \\cite{feng2021twibot} & 3632 & 4646 & Crowdsourced annotations.\\\\\n",
      "\\data{vendor-purchased-2019} & \\cite{yang2019arming} & 0 & 1087 & Fake followers purchased from three companies.\\\\\n"
     ]
    }
   ],
   "source": [
    "for row in dataset_df.to_dict(orient=\"records\"):\n",
    "    if not row['analyzed?']:\n",
    "        continue\n",
    "    name = \"\\\\data{\" + row['dataset name'] + \"}\"\n",
    "#     if row['aggregated benchmark dataset']:\n",
    "#         name = name + \"\\\\data{\" + f\" ({row['aggregated benchmark dataset']})\" + \"}\"\n",
    "        \n",
    "    num_humans = int(row['# humans we have']) if row['# humans we have']!=\"\" else '-'\n",
    "    num_bots = int(row['# bots we have']) if row['# bots we have']!=\"\" else '-'\n",
    "    #print(name + \" & \\\\cite{\" + row['bibtex_id'] + \"} & \" + f\"{num_users} & {prop_bots:.2f} \\\\\\\\\")\n",
    "    print(\n",
    "        name + \\\n",
    "        \" & \\\\cite{\" + \\\n",
    "        row['bibtex_id'] + \\\n",
    "        \"} \" + \\\n",
    "        f\"& {num_humans} & {num_bots} & {row['description']}\\\\\\\\\"\n",
    "    )\n",
    "    if row['aggregated benchmark dataset']:\n",
    "        print(\"\\\\data{\" + f\" ({row['aggregated benchmark dataset']})\" + \"} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804cc37",
   "metadata": {},
   "source": [
    "## Generate table for dataset -> sdt/sota scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63cf430a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T02:29:18.218213Z",
     "start_time": "2022-10-02T02:29:18.213747Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_score(score, dataset_name):\n",
    "    if score == \"\":\n",
    "        return -1\n",
    "    match = re.search(f\"all: ([0-9]*(\\.\\d+)?)\", score)\n",
    "    if match:\n",
    "        return -1\n",
    "    match = re.search(f\"{dataset_name}: ([0-9]*(\\.\\d+)?)\", score)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ef715b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T02:29:18.462503Z",
     "start_time": "2022-10-02T02:29:18.458841Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_score(df, dataset_name, metric):\n",
    "    scores = df[df['dataset(s) used'].str.contains(dataset_name)][metric].map(lambda x: get_score(x, dataset_name))\n",
    "    max_score_ind = scores.idxmax()\n",
    "    return scores.loc[max_score_ind], df.at[max_score_ind, 'bibtex_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f30504b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T02:29:18.692444Z",
     "start_time": "2022-10-02T02:29:18.676043Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    'botometer-feedback-2019',\n",
    "    'caverlee-2011',\n",
    "    'cresci-2015',\n",
    "    'cresci-2017',\n",
    "    'cresci-rtbust-2019',\n",
    "    #'cresci-stock-2018',\n",
    "    #'yang-2013',\n",
    "    'gilani-2017',\n",
    "    'midterm-2018',\n",
    "    'pan-2019',\n",
    "    'twibot-2020',\n",
    "    'varol-2017'\n",
    "]\n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "for name in dataset_names:\n",
    "    score_dict[name] = {\n",
    "        'accuracy': get_max_score(df, name, 'accuracy'),\n",
    "        'f1': get_max_score(df, name, 'f1')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cc2178b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T02:29:18.867920Z",
     "start_time": "2022-10-02T02:29:18.862320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'botometer-feedback-2019': {'accuracy': (0.8108, 'guo2022social'),\n",
       "  'f1': (0.6977, 'guo2022social')},\n",
       " 'caverlee-2011': {'accuracy': (0.9862, 'lee2011a'),\n",
       "  'f1': (0.986, 'lee2011a')},\n",
       " 'cresci-2015': {'accuracy': (0.991, 'cresci2015fame'),\n",
       "  'f1': (0.991, 'cresci2015fame')},\n",
       " 'cresci-2017': {'accuracy': (0.9981, 'kudugunta2018deep'),\n",
       "  'f1': (1.0, 'kudugunta2018deep')},\n",
       " 'cresci-rtbust-2019': {'accuracy': (0.9304, 'mazza2019rtbust'),\n",
       "  'f1': (0.8687, 'mazza2019rtbust')},\n",
       " 'gilani-2017': {'accuracy': (0.8644, 'gilani2020classification'),\n",
       "  'f1': (0.836, 'gilani2020classification')},\n",
       " 'midterm-2018': {'accuracy': (0.964, 'antenore2022a'),\n",
       "  'f1': (0.9413, 'ng2023botbuster')},\n",
       " 'pan-2019': {'accuracy': (0.9509, 'geng2021satar'),\n",
       "  'f1': (0.951, 'geng2021satar')},\n",
       " 'twibot-2020': {'accuracy': (0.8664, 'feng2022heterogeneity-aware'),\n",
       "  'f1': (0.8821, 'feng2022heterogeneity-aware')},\n",
       " 'varol-2017': {'accuracy': (-1, 'dimitriadis2021social'),\n",
       "  'f1': (0.7306, 'kosmajac2019twitter')}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b29a862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-02T02:36:48.467075Z",
     "start_time": "2022-10-02T02:36:48.442126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "botometer-feedback-2019 & 0.78 \\textit{(4)} & 0.78  \\textit{(5)} & 0.81 \\cite{guo2022social} & 0.70 \\cite{guo2022social} & -0.03 & 0.08 \\\\\n",
      "caverlee-2011 & 0.93 \\textit{(4)} & 0.93  \\textit{(4)} & 0.99 \\cite{lee2011a} & 0.99 \\cite{lee2011a} & -0.06 & -0.06 \\\\\n",
      "cresci-2015 & 0.97 \\textit{(3)} & 0.97  \\textit{(3)} & 0.99 \\cite{cresci2015fame} & 0.99 \\cite{cresci2015fame} & -0.02 & -0.02 \\\\\n",
      "cresci-2017 & 0.98 \\textit{(1)} & 0.98  \\textit{(1)} & 1.00 \\cite{kudugunta2018deep} & 1.00 \\cite{kudugunta2018deep} & -0.02 & -0.02 \\\\\n",
      "cresci-rtbust-2019 & 0.72 \\textit{(4)} & 0.72  \\textit{(1)} & 0.93 \\cite{mazza2019rtbust} & 0.87 \\cite{mazza2019rtbust} & -0.21 & -0.15 \\\\\n",
      "gilani-2017 & 0.81 \\textit{(1)} & 0.81  \\textit{(2)} & 0.86 \\cite{gilani2020classification} & 0.84 \\cite{gilani2020classification} & -0.05 & -0.03 \\\\\n",
      "midterm-2018 & 0.97 \\textit{(1)} & 0.97  \\textit{(1)} & 0.96 \\cite{antenore2022a} & 0.94 \\cite{ng2023botbuster} & 0.01 & 0.03 \\\\\n",
      "pan-2019 & 0.93 \\textit{(3)} & 0.93  \\textit{(3)} & 0.95 \\cite{geng2021satar} & 0.95 \\cite{geng2021satar} & -0.02 & -0.03 \\\\\n",
      "twibot-2020 & 0.82 \\textit{(1)} & 0.82  \\textit{(1)} & 0.87 \\cite{feng2022heterogeneity-aware} & 0.88 \\cite{feng2022heterogeneity-aware} & -0.05 & -0.07 \\\\\n"
     ]
    }
   ],
   "source": [
    "max_depth = 5\n",
    "tolerance = 0.025\n",
    "\n",
    "for k, v in score_dict.items():\n",
    "    #print(k)\n",
    "    if k == 'varol-2017':\n",
    "        continue\n",
    "    accuracy_sota = float(v['accuracy'][0])\n",
    "    f1_sota = float(v['f1'][0])\n",
    "    \n",
    "    row = sdt_df[sdt_df['name'] == k].to_dict(orient=\"records\")[0]\n",
    "    accuracies = [row[f'a{i}'] for i in range(1, max_depth+1)]\n",
    "    a_max_ind = np.argmax(accuracies)\n",
    "    f1s = [row[f'f{i}'] for i in range(1, max_depth+1)]\n",
    "    f_max_ind = np.argmax(f1s)\n",
    "    accuracy_sdt = accuracies[a_max_ind]\n",
    "    f1_sdt = f1s[f_max_ind]\n",
    "    \n",
    "    for i, acc in enumerate(accuracies):\n",
    "        if accuracy_sdt - acc <= tolerance:\n",
    "            a_max_ind = i\n",
    "            accuracy_sdt = acc\n",
    "            break\n",
    "    for i, f in enumerate(f1s):\n",
    "        if f1_sdt - f <= tolerance:\n",
    "            f_max_ind = i\n",
    "            f1_sdt = acc\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    accuracy_diff = accuracy_sdt - accuracy_sota\n",
    "    f1_diff = f1_sdt - f1_sota\n",
    "        \n",
    "    print(f\"{k} & {accuracy_sdt:0.2f}\" \\\n",
    "          + \" \\\\textit{\" \\\n",
    "          + f\"({a_max_ind+1})\" \\\n",
    "          + \"} & \" \\\n",
    "          + f\"{f1_sdt:0.2f} \" \\\n",
    "          \" \\\\textit{\" \\\n",
    "          + f\"({f_max_ind+1})\" \\\n",
    "          + \"} & \" \\\n",
    "          + f\"{accuracy_sota:0.2f}\" \\\n",
    "          + \" \\\\cite{\" + f\"{v['accuracy'][1]}\" \\\n",
    "          + \"} & \" \\\n",
    "          + f\"{f1_sota:0.2f}\" \\\n",
    "          + \" \\\\cite{\" \\\n",
    "          + f\"{v['f1'][1]}\" + \"} & \" \\\n",
    "          + f\"{accuracy_diff:0.2f} & {f1_diff:0.2f} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964d0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
