{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99af18c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T23:11:26.174138Z",
     "start_time": "2022-10-06T23:11:25.863260Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from biblib import Entry\n",
    "import pybtex as pbt\n",
    "import math\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec8925",
   "metadata": {},
   "source": [
    "## Load data needed to generate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec96f398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T23:11:34.828940Z",
     "start_time": "2022-10-06T23:11:34.817816Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '~/Downloads/Simple Decision Rules Give High Accuracy for Detecting Social Bots on Benchmark Datasets - Bot detection papers(18).tsv'\n",
    "#path = 'metrics.tsv'\n",
    "\n",
    "df = pd.read_csv(path, sep='\\t')\n",
    "df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cd0939d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T02:27:17.102659Z",
     "start_time": "2022-10-07T02:27:17.092531Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df_path = '~/Downloads/Simple Decision Rules Give High Accuracy for Detecting Social Bots on Benchmark Datasets - datasets(15).tsv'\n",
    "\n",
    "dataset_df = pd.read_csv(dataset_df_path, sep='\\t')\n",
    "dataset_df.fillna(\"\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf58bb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T23:11:35.408751Z",
     "start_time": "2022-10-06T23:11:35.397867Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_path = '~/work/repo/bot-detection/scores.csv'\n",
    "sdt_df = pd.read_csv(scores_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfdb940",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate bibliography methods.bib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5477a63e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T23:11:36.446857Z",
     "start_time": "2022-10-06T23:11:36.382445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bib = pbt.database.BibliographyData()\n",
    "\n",
    "def add_bib_entries(df):\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        #print(row)\n",
    "        if row['bibtex_id'] in bib.entries.keys():\n",
    "            continue\n",
    "        if 'analyzed?' in row:\n",
    "            if not row['analyzed?']:\n",
    "                continue\n",
    "        if not row['bibtex_id']:\n",
    "            continue\n",
    "        inputdict = {\n",
    "            'author': row['authors'],\n",
    "            'title': row['title'],\n",
    "            'year': str(int(row['year']))\n",
    "        }\n",
    "\n",
    "        if row['conference?']:\n",
    "            inputdict.update({\n",
    "                'booktitle': row['booktitle'],\n",
    "                'pages': row['pages'],\n",
    "            })\n",
    "            if row['booktitle']:\n",
    "                inputdict['booktitle'] = row['booktitle'] \n",
    "            type_ = 'inproceedings'\n",
    "        else:\n",
    "            inputdict['journal'] = row['journal']\n",
    "            if row['volume']:\n",
    "                inputdict['volume'] = str(int(row['volume']))\n",
    "            if row['number']:\n",
    "                inputdict['number'] = str(int(row['number']))\n",
    "            type_ = 'article'\n",
    "        if row['publisher']:\n",
    "            inputdict['publisher'] = row['publisher']\n",
    "        if row['doi']:\n",
    "            inputdict['doi'] = row['doi']\n",
    "        if row['pages']:\n",
    "            inputdict['pages'] = row['pages']\n",
    "        entry = pbt.database.Entry(type_=type_, fields=inputdict)\n",
    "        bib.add_entry(entry=entry, key=row['bibtex_id'])\n",
    "\n",
    "add_bib_entries(df)\n",
    "add_bib_entries(dataset_df)\n",
    "\n",
    "bib.to_file(\"methods.bib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620287bc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate table dataset -> paper that uses it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd6e603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T19:47:21.098519Z",
     "start_time": "2022-10-01T19:47:21.092409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "\n",
    "for row in df.to_dict(orient=\"records\"):\n",
    "    datasets = row['dataset(s) used'].split(\"; \")\n",
    "    for d in datasets:\n",
    "        if d in dataset_dict:\n",
    "            dataset_dict[d].append(row['bibtex_id'])\n",
    "        else:\n",
    "            dataset_dict[d] = [row['bibtex_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85826edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T19:47:21.850074Z",
     "start_time": "2022-10-01T19:47:21.840011Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twibot-2020 & \\cite{feng2022heterogeneity-aware, , , feng2021botrgcn, geng2021satar, dehghan2018detecting} \\\\\n",
      "cresci-rtbust-2019 & \\cite{guo2022social, , yang2020scalable, sayyadiharikandeh2020detection, , , , mazza2019rtbust} \\\\\n",
      "botometer-feedback-2019 & \\cite{guo2022social, , yang2020scalable, sayyadiharikandeh2020detection} \\\\\n",
      "gilani-2017 & \\cite{guo2022social, dimitriadis2021social, , yang2020scalable, gilani2020classification, sayyadiharikandeh2020detection, , echeverria2018lobo} \\\\\n",
      "cresci-stock-2018 & \\cite{guo2022social, dimitriadis2021social, , , yang2020scalable, sayyadiharikandeh2020detection} \\\\\n",
      "midterm-2018 & \\cite{guo2022social, dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "cresci-2015 & \\cite{gonzalez2022the, , dimitriadis2021social, , stella2019influence, echeverria2018lobo, cresci2015fame} \\\\\n",
      "cresci-2017 & \\cite{gonzalez2022the, thavasimani2022a, heidari2021an, ilias2021detecting, geng2021satar, dimitriadis2021social, , yang2020scalable, heidari2020using, heidari2020deep, sayyadiharikandeh2020detection, , stella2019influence, loyola-gonzalez2019contrast, mohammad2019bot, alhosseini2019detect, knauth2019language-agnostic, kosmajac2019twitter, , kudugunta2018deep, cresci2018social, efthimion2018supervised, echeverria2018lobo, ferrara2017disinformation, cresci2017the, cresci2017exploiting, cresci2016dna-inspired} \\\\\n",
      " & \\cite{, rodrÃ­guez-ruiza2020a, braker2020botspot, , , zheng2015detecting, stringhini2010detecting, lee2010uncovering, cresci2018from, chavoshi2017on-demand, gilani2019a, gilani2017of} \\\\\n",
      "caverlee-2011 & \\cite{ilias2021detecting, dimitriadis2021social, yang2020scalable, sayyadiharikandeh2020detection, , alhosseini2019detect, beskow2018bot, varol2017online, wu2017adaptive, davis2016botornot, alarifi2016twitter, lee2011a} \\\\\n",
      "pan-2019 & \\cite{geng2021satar, , luo2019deepbot} \\\\\n",
      "pronbots-2019 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "political-bots-2019 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection} \\\\\n",
      "astroturf & \\cite{dimitriadis2021social, sayyadiharikandeh2020detection} \\\\\n",
      "botwiki-2019 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "vendor-purchased-2019 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "varol-2017 & \\cite{dimitriadis2021social, , yang2020scalable, sayyadiharikandeh2020detection, , kosmajac2019twitter, varol2017online, ferrara2017disinformation} \\\\\n",
      "italian_elections & \\cite{dimitriadis2021social} \\\\\n",
      "ira-2018 & \\cite{weber2021amplifying} \\\\\n",
      "weber-2020 & \\cite{weber2021amplifying} \\\\\n",
      "verified-2019 & \\cite{, yang2020scalable, sayyadiharikandeh2020detection, } \\\\\n",
      "celebrity-2019 & \\cite{, } \\\\\n",
      "cresci-rtbust & \\cite{} \\\\\n",
      "botometer-feedback & \\cite{} \\\\\n",
      "gilani & \\cite{} \\\\\n",
      "midterm & \\cite{} \\\\\n",
      "kaiser & \\cite{sayyadiharikandeh2020detection} \\\\\n",
      "vargas-2020 & \\cite{vargas2020on} \\\\\n",
      "policial-bots-2019 & \\cite{} \\\\\n",
      "russian-trolls-2018 & \\cite{stella2019influence} \\\\\n",
      "cresci-2017 tweets & \\cite{wei2019twitter} \\\\\n",
      "yang-2013-traditional-spambots-1 & \\cite{alhosseini2019detect} \\\\\n",
      "albadi-2018 & \\cite{albadi2019investigating, albadi2018are} \\\\\n",
      "gilani-2017 tweets & \\cite{garcia-silva2019an} \\\\\n",
      "NBC-2018 & \\cite{efthimion2018supervised} \\\\\n",
      "random-strings-2018 & \\cite{beskow2018bot} \\\\\n",
      "journalist-attack-2017 & \\cite{beskow2018bot, echeverria2018lobo} \\\\\n",
      "starwars & \\cite{echeverria2018lobo} \\\\\n",
      "besel-2018 & \\cite{echeverria2018lobo} \\\\\n",
      "debot & \\cite{echeverria2018lobo, chavoshi2016debot} \\\\\n",
      "darpa-challenge-2015 & \\cite{echeverria2018lobo} \\\\\n",
      "mesnards-2018 & \\cite{mesnards2018detecting} \\\\\n",
      "morstatter-2016 & \\cite{cai2017behavior, morstatter2016a} \\\\\n",
      "ersahin-2017 & \\cite{ersahin2017twitter, ahmed2013a} \\\\\n",
      "kantepe-2017 & \\cite{kantepe2017preprocessing} \\\\\n",
      "alarifi-2016 & \\cite{alarifi2016twitter} \\\\\n",
      "wang-2009 & \\cite{miller2014twitter, wang2010dont} \\\\\n",
      "a large dataset relating to the 2014 Indian election & \\cite{dickerson2014using} \\\\\n",
      "yang-2013-traditional-spambots & \\cite{yang2013empirical} \\\\\n",
      "chu-2012 & \\cite{chu2012detecting2, chu2012detecting} \\\\\n",
      "possibly spammers used in cresci-2017 or another spam dataset & \\cite{chao2011die} \\\\\n"
     ]
    }
   ],
   "source": [
    "for k,v in dataset_dict.items():\n",
    "    impl_papers = \", \".join(dataset_dict[k])\n",
    "\n",
    "    cite_as = '\\\\cite{' + impl_papers + '} \\\\\\\\'\n",
    "    dataset_name = k\n",
    "    print(dataset_name+ \" & \" + cite_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ca18b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate table for dataset, #people/bots, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "206f8a3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T02:28:33.567201Z",
     "start_time": "2022-10-07T02:28:33.555646Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data{botwiki-2019} & \\cite{yang2020scalable} & 0 & 698 & Labeled datasets available in the literature and three new ones.\\\\\n",
      "\\data{celebrity-2019} & \\cite{yang2019arming} & 5918 & 0 & Celebrity accounts.\\\\\n",
      "\\data{the-fake-project-2015} & \\cite{cresci2015fame} & 469 & 0 & Followers of @TheFakeProject who completed unique CAPTCHA.\\\\\n",
      "\\data{elezioni-2015} & \\cite{cresci2015fame} & 1488 & 0 & Users of \\#elezioni2013, excluding political actors and media; manually verified.\\\\\n",
      "\\data{fake-followers-2015} & \\cite{cresci2017the} & 0 & 3351 & Accounts that inflate another account's number of followers.\\\\\n",
      "\\data{genuine-accounts-cresci} & \\cite{cresci2017the} & 3474 & 0 & Random\\footnote{We describe in \\Cref{sec:background} our reason for believing this sample is a convenience sample from previous work.} sample of verified, human-operated accounts through hybrid crowdsensing.\\\\\n",
      "\\data{social-spambots-1} & \\cite{cresci2017the} & 0 & 991 & Retweeters of an Italian political candidate during 2014 campaign.\\\\\n",
      "\\data{social-spambots-2} & \\cite{cresci2017the} & 0 & 3457 & Spammers promoting the \\#TALNTS for several months.\\\\\n",
      "\\data{social-spambots-3} & \\cite{cresci2017the} & 0 & 464 & Spammers of advertised products on Amazon.\\\\\n",
      "\\data{traditional-spambots-yang} & \\cite{yang2013empirical} & 0 & 1000 & Accounts spamming malicious links from public blacklists.\\\\\n",
      "\\data{genuine-accounts-yang} & \\cite{yang2013empirical} & 10000 & 0 & Random sample of genuine (human-operated) accounts.\\\\\n",
      "\\data{traditional-spambots-2} & \\cite{cresci2017the} & 0 & 100 & Accounts repeatedly mentioning other users in tweets with scam URLs.\\\\\n",
      "\\data{traditional-spambots-3} & \\cite{cresci2017the} & 0 & 403 & Automated accounts spamming job offers.\\\\\n",
      "\\data{traditional-spambots-4} & \\cite{cresci2017the} & 0 & 1128 & Automated accounts spamming job offers.\\\\\n",
      "\\data{political-bots-2019} & \\cite{yang2019arming} & 0 & 62 & Automated political Twitter accounts run by @rzazula, shared by @josh\\_emerson.\\\\\n",
      "\\data{pronbots-2019} & \\cite{yang2019arming} & 0 & 17882 & Twitter bots sharing scam sites; collected in May 2018.\\\\\n",
      "\\data{vendor-purchased-2019} & \\cite{yang2019arming} & 0 & 1087 & Fake followers purchased from three companies.\\\\\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "benchmark_only = False\n",
    "\n",
    "for row in dataset_df.to_dict(orient=\"records\"):\n",
    "    if not row['analyzed?']:\n",
    "        continue\n",
    "    if benchmark_only and row['benchmark?'] != '1':\n",
    "        continue\n",
    "    if not benchmark_only and row['benchmark?'] == '1':\n",
    "        continue\n",
    "    name = \"\\\\data{\" + row['dataset name'] + \"}\"\n",
    "        \n",
    "    num_humans = int(row['# humans we have']) if row['# humans we have']!=\"\" else '-'\n",
    "    num_bots = int(row['# bots we have']) if row['# bots we have']!=\"\" else '-'\n",
    "    \n",
    "    if benchmark_only:\n",
    "        desc_list = row['description'].split(\"; \")\n",
    "        if len(desc_list) == 1:\n",
    "            desc = row['description']\n",
    "        else:\n",
    "            desc_list = ['\\\\data{' + d + '}' for d in desc_list]\n",
    "            desc = \", \".join(desc_list)\n",
    "    else:\n",
    "        desc = row['description']\n",
    "    print(\n",
    "        name + \\\n",
    "        \" & \\\\cite{\" + \\\n",
    "        row['bibtex_id'] + \\\n",
    "        \"} \" + \\\n",
    "        f\"& {num_humans} & {num_bots} & {desc}\\\\\\\\\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804cc37",
   "metadata": {},
   "source": [
    "## Generate table for dataset -> sdt/sota scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cf430a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T02:42:11.651834Z",
     "start_time": "2022-10-05T02:42:11.646020Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_score(score, dataset_name):\n",
    "    if score == \"\":\n",
    "        return -1\n",
    "    match = re.search(f\"all: ([0-9]*(\\.\\d+)?)\", score)\n",
    "    if match:\n",
    "        return -1\n",
    "    match = re.search(f\"{dataset_name}: ([0-9]*(\\.\\d+)?)\", score)\n",
    "    if match: \n",
    "        return float(match.group(1))\n",
    "    return float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef715b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T02:42:11.816235Z",
     "start_time": "2022-10-05T02:42:11.812795Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_score(df, dataset_name, metric):\n",
    "    scores = df[df['dataset(s) used'].str.contains(dataset_name)][metric].map(lambda x: get_score(x, dataset_name))\n",
    "    max_score_ind = scores.idxmax()\n",
    "    return scores.loc[max_score_ind], df.at[max_score_ind, 'bibtex_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e045fc57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T02:42:11.996440Z",
     "start_time": "2022-10-05T02:42:11.977721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34   -1.0\n",
       "67    0.9\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['dataset(s) used'].str.contains('yang-2013')]['f1'].map(lambda x: get_score(x, 'yang-2013'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f30504b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T02:58:14.243394Z",
     "start_time": "2022-10-05T02:58:14.229457Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \n",
    "    'caverlee-2011',\n",
    "    'cresci-2015',\n",
    "    'cresci-2017',\n",
    "    #'yang-2013',\n",
    "    'botometer-feedback-2019',\n",
    "    'midterm-2018',\n",
    "    'cresci-rtbust-2019',\n",
    "    'pan-2019',\n",
    "    'gilani-2017',\n",
    "    #'cresci-stock-2018',\n",
    "    'twibot-2020',\n",
    "]\n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "for name in dataset_names:\n",
    "    score_dict[name] = {\n",
    "        'accuracy': get_max_score(df, name, 'accuracy'),\n",
    "        'f1': get_max_score(df, name, 'f1'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b29a862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T02:58:17.717443Z",
     "start_time": "2022-10-05T02:58:17.699655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data{caverlee-2011} & 0.93/0.91 & 2 &  \\cite{lee2011a} & -0.06/-0.07 \\\\\n",
      "\\data{cresci-2015} & 0.98/0.98 & 3 &  \\cite{cresci2015fame} & -0.01/-0.01 \\\\\n",
      "\\data{cresci-2017} & 0.98/0.98 & 1 &  \\cite{kudugunta2018deep} & -0.02/-0.02 \\\\\n",
      "\\data{botometer-feedback-2019} & 0.79/0.56 & 4 &  \\cite{guo2022social} & -0.02/-0.14 \\\\\n",
      "\\data{midterm-2018} & 0.97/0.98 & 1 &  \\cite{giorgi2021a} & -0.01/\\phantom{-}0.01 \\\\\n",
      "\\data{cresci-rtbust-2019} & 0.69/0.73 & 1 &  \\cite{mazza2019rtbust} & -0.24/-0.14 \\\\\n",
      "\\data{pan-2019} & 0.93/0.93 & 3 &  \\cite{geng2021satar} & -0.02/-0.03 \\\\\n",
      "\\data{gilani-2017} & 0.81/0.80 & 2 &  \\cite{gilani2020classification} & -0.05/-0.03 \\\\\n",
      "\\data{twibot-2020} & 0.82/0.86 & 1 &  \\cite{feng2022heterogeneity-aware} & -0.05/-0.03 \\\\\n"
     ]
    }
   ],
   "source": [
    "def print_single_dataset_score_table(score_dict):\n",
    "    max_depth = 5\n",
    "    tolerance = 0.025\n",
    "\n",
    "    for k,v in score_dict.items():\n",
    "        accuracy_sota = float(v['accuracy'][0])\n",
    "        f1_sota = float(v['f1'][0])\n",
    "        row = sdt_df[sdt_df['name'] == k].to_dict(orient=\"records\")[0]\n",
    "        accuracies = [row[f'a{i}'] for i in range(1, max_depth+1)]\n",
    "        a_max_ind = np.argmax(accuracies)\n",
    "        f1s = [row[f'f{i}'] for i in range(1, max_depth+1)]\n",
    "        f_max_ind = np.argmax(f1s)\n",
    "        accuracy_sdt = accuracies[a_max_ind]\n",
    "        f1_sdt = f1s[f_max_ind]\n",
    "\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            if accuracy_sdt - acc <= tolerance:\n",
    "                a_max_ind = i\n",
    "                accuracy_sdt = acc\n",
    "                break\n",
    "        for i, f in enumerate(f1s):\n",
    "            if f1_sdt - f <= tolerance:\n",
    "                f_max_ind = i\n",
    "                f1_sdt = f\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        accuracy_diff = accuracy_sdt - accuracy_sota\n",
    "        f1_diff = f1_sdt - f1_sota\n",
    "        if v['accuracy'][1] == v['f1'][1]:\n",
    "            cite = f\"{v['accuracy'][1]}\"\n",
    "        else:\n",
    "            cite = f\"{v['accuracy'][1]}, {v['f1'][1]}\"\n",
    "        sepa = '\\\\phantom{-}' if accuracy_diff > 0 else ''\n",
    "        sepf = '\\\\phantom{-}' if f1_diff > 0 else ''\n",
    "        print('\\\\data{' + f\"{k}\" + \"} & \" + f\"{accuracy_sdt:0.2f}\" + f\"/{f1_sdt:0.2f} \" \\\n",
    "    #           + \" \\\\textit{\" \\\n",
    "    #           + f\"({a_max_ind+1})\" \\\n",
    "    #           + \"} & \" \\\n",
    "              + f\"& {f_max_ind+1}\" \\\n",
    "              + \" & \" \\\n",
    "    #           + f\"{accuracy_sota:0.2f}\" \\\n",
    "    #           + \" \\\\cite{\" + f\"{v['accuracy'][1]}\" \\\n",
    "    #           + \"} & \" \\\n",
    "    #           + f\"{f1_sota:0.2f}\" \\\n",
    "              + \" \\\\cite{\" \\\n",
    "              + cite + \"} & \" \\\n",
    "              + f\"{sepa}{accuracy_diff:0.2f}/{sepf}{f1_diff:0.2f} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in leave_one_out_dict.items():\n",
    "    accuracy_sota = float(v['accuracy'][0])\n",
    "    f1_sota = float(v['f1'][0])\n",
    "    row = sdt_df[sdt_df['name'] == k].to_dict(orient=\"records\")[0]\n",
    "    accuracies = [row[f'a{i}'] for i in range(1, max_depth+1)]\n",
    "    a_max_ind = np.argmax(accuracies)\n",
    "    f1s = [row[f'f{i}'] for i in range(1, max_depth+1)]\n",
    "    f_max_ind = np.argmax(f1s)\n",
    "    accuracy_sdt = accuracies[a_max_ind]\n",
    "    f1_sdt = f1s[f_max_ind]\n",
    "    \n",
    "    for i, acc in enumerate(accuracies):\n",
    "        if accuracy_sdt - acc <= tolerance:\n",
    "            a_max_ind = i\n",
    "            accuracy_sdt = acc\n",
    "            break\n",
    "    for i, f in enumerate(f1s):\n",
    "        if f1_sdt - f <= tolerance:\n",
    "            f_max_ind = i\n",
    "            f1_sdt = f\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    accuracy_diff = accuracy_sdt - accuracy_sota\n",
    "    f1_diff = f1_sdt - f1_sota\n",
    "    if v['accuracy'][1] == v['f1'][1]:\n",
    "        cite = f\"{v['accuracy'][1]}\"\n",
    "    else:\n",
    "        cite = f\"{v['accuracy'][1]}, {v['f1'][1]}\"\n",
    "    sepa = '\\\\phantom{-}' if accuracy_diff > 0 else ''\n",
    "    sepf = '\\\\phantom{-}' if f1_diff > 0 else ''\n",
    "    print('\\\\data{' + f\"{k}\" + \"} & \" + f\"{accuracy_sdt:0.2f}\" + f\"/{f1_sdt:0.2f} \" \\\n",
    "#           + \" \\\\textit{\" \\\n",
    "#           + f\"({a_max_ind+1})\" \\\n",
    "#           + \"} & \" \\\n",
    "          + f\"& {f_max_ind+1}\" \\\n",
    "          + \" & \" \\\n",
    "#           + f\"{accuracy_sota:0.2f}\" \\\n",
    "#           + \" \\\\cite{\" + f\"{v['accuracy'][1]}\" \\\n",
    "#           + \"} & \" \\\n",
    "#           + f\"{f1_sota:0.2f}\" \\\n",
    "          + \" \\\\cite{\" \\\n",
    "          + cite + \"} & \" \\\n",
    "          + f\"{sepa}{accuracy_diff:0.2f}/{sepf}{f1_diff:0.2f} \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
